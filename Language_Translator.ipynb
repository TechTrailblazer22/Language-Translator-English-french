{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "aJ8f4NlMnqmt"
      },
      "outputs": [],
      "source": [
        "from gensim.models import Word2Vec\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, LSTM, Dense, Embedding\n",
        "from tensorflow.keras.optimizers import RMSprop\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "\n",
        "# Parameters\n",
        "batch_size = 64\n",
        "epochs = 100\n",
        "latent_dim = 256\n",
        "num_samples = 10000\n",
        "data_path = 'fra.txt'\n",
        "embedding_dim = 300  # Should match with the Word2Vec embedding dimension"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train your own Word2Vec model\n",
        "sentences = [line.split('\\t')[0].split() for line in open(data_path, 'r', encoding='utf-8').read().split('\\n') if '\\t' in line]\n",
        "\n",
        "word2vec_model = Word2Vec(sentences, vector_size=embedding_dim, window=5, min_count=1, workers=4)\n",
        "word2vec_model.save('custom_word2vec.model')\n",
        "\n",
        "# Load your custom Word2Vec model\n",
        "custom_word2vec_model = Word2Vec.load('custom_word2vec.model')"
      ],
      "metadata": {
        "id": "53bmltT2oOGz"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare Data\n",
        "input_texts = []\n",
        "target_texts = []\n",
        "input_words = set()\n",
        "target_words = set()\n",
        "\n",
        "with open(data_path, 'r', encoding='utf-8') as f:\n",
        "    lines = f.read().split('\\n')\n",
        "\n",
        "for line in lines[: min(num_samples, len(lines) - 1)]:\n",
        "    input_text, target_text, _ = line.split('\\t')\n",
        "    target_text = '\\t' + target_text + '\\n'\n",
        "    input_texts.append(input_text)\n",
        "    target_texts.append(target_text)\n",
        "    input_words.update(input_text.split())\n",
        "    target_words.update(target_text.split())\n",
        "\n",
        "input_words = sorted(list(input_words))\n",
        "target_words = sorted(list(target_words))\n",
        "num_encoder_tokens = len(input_words)\n",
        "num_decoder_tokens = len(target_words)\n",
        "max_encoder_seq_length = max([len(txt.split()) for txt in input_texts])\n",
        "max_decoder_seq_length = max([len(txt.split()) for txt in target_texts])\n",
        "\n",
        "print('Number of samples:', len(input_texts))\n",
        "print('Number of unique input tokens:', num_encoder_tokens)\n",
        "print('Number of unique output tokens:', num_decoder_tokens)\n",
        "print('Max sequence length for inputs:', max_encoder_seq_length)\n",
        "print('Max sequence length for outputs:', max_decoder_seq_length)\n",
        "\n",
        "input_token_index = dict([(word, i) for i, word in enumerate(input_words)])\n",
        "target_token_index = dict([(word, i) for i, word in enumerate(target_words)])"
      ],
      "metadata": {
        "id": "wLmrslyyoX_B",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eaef7f2b-e1ad-422c-998a-cc06c1ab81ae"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of samples: 10000\n",
            "Number of unique input tokens: 2939\n",
            "Number of unique output tokens: 5731\n",
            "Max sequence length for inputs: 5\n",
            "Max sequence length for outputs: 10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_embedding_matrix(word_index, embedding_model):\n",
        "    embedding_matrix = np.zeros((len(word_index) + 1, embedding_dim))\n",
        "    for word, i in word_index.items():\n",
        "        if word in embedding_model.wv:\n",
        "            embedding_matrix[i] = embedding_model.wv[word]\n",
        "        else:\n",
        "            embedding_matrix[i] = np.random.normal(size=(embedding_dim,))\n",
        "    return embedding_matrix"
      ],
      "metadata": {
        "id": "-ZjoBK3QogLn"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_matrix_input = get_embedding_matrix(input_token_index, custom_word2vec_model)\n",
        "embedding_matrix_target = get_embedding_matrix(target_token_index, custom_word2vec_model)\n",
        "\n",
        "encoder_input_data = np.zeros((len(input_texts), max_encoder_seq_length), dtype='int32')\n",
        "decoder_input_data = np.zeros((len(input_texts), max_decoder_seq_length), dtype='int32')\n",
        "decoder_target_data = np.zeros((len(input_texts), max_decoder_seq_length, num_decoder_tokens), dtype='float32')"
      ],
      "metadata": {
        "id": "xj8g9vCPojOz"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i, (input_text, target_text) in enumerate(zip(input_texts, target_texts)):\n",
        "    for t, word in enumerate(input_text.split()):\n",
        "        if word in input_token_index:\n",
        "            encoder_input_data[i, t] = input_token_index[word]\n",
        "    for t, word in enumerate(target_text.split()):\n",
        "        if word in target_token_index:\n",
        "            decoder_input_data[i, t] = target_token_index[word]\n",
        "            if t > 0:\n",
        "                decoder_target_data[i, t - 1, target_token_index[word]] = 1."
      ],
      "metadata": {
        "id": "UkrVP6KMo_Xw"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split data into training, validation, and test sets\n",
        "train_size = 0.8\n",
        "validation_size = 0.1\n",
        "test_size = 0.1\n",
        "\n",
        "encoder_input_data_train, encoder_input_data_temp, decoder_input_data_train, decoder_input_data_temp, decoder_target_data_train, decoder_target_data_temp = train_test_split(\n",
        "    encoder_input_data, decoder_input_data, decoder_target_data, train_size=train_size, random_state=42)\n",
        "\n",
        "validation_proportion = validation_size / (validation_size + test_size)\n",
        "\n",
        "encoder_input_data_val, encoder_input_data_test, decoder_input_data_val, decoder_input_data_test, decoder_target_data_val, decoder_target_data_test = train_test_split(\n",
        "    encoder_input_data_temp, decoder_input_data_temp, decoder_target_data_temp, train_size=validation_proportion, random_state=42)"
      ],
      "metadata": {
        "id": "IcIYn-ZNoyTd"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create embedding layers\n",
        "embedding_layer_input = Embedding(input_dim=num_encoder_tokens + 1, output_dim=embedding_dim,\n",
        "                                  weights=[embedding_matrix_input], input_length=max_encoder_seq_length, trainable=False)\n",
        "embedding_layer_target = Embedding(input_dim=num_decoder_tokens + 1, output_dim=embedding_dim,\n",
        "                                   weights=[embedding_matrix_target], input_length=max_decoder_seq_length, trainable=False)"
      ],
      "metadata": {
        "id": "RdFK-BAPo2vz"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split data into training, validation, and test sets\n",
        "train_size = 0.8\n",
        "validation_size = 0.1\n",
        "test_size = 0.1\n",
        "\n",
        "input_texts_train, input_texts_temp, target_texts_train, target_texts_temp = train_test_split(\n",
        "    input_texts, target_texts, train_size=train_size, random_state=42)\n",
        "\n",
        "validation_proportion = validation_size / (validation_size + test_size)\n",
        "\n",
        "input_texts_val, input_texts_test, target_texts_val, target_texts_test = train_test_split(\n",
        "    input_texts_temp, target_texts_temp, train_size=validation_proportion, random_state=42)"
      ],
      "metadata": {
        "id": "SKshaTWja_hf"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model building\n",
        "encoder_inputs = Input(shape=(max_encoder_seq_length,))\n",
        "encoder_embeddings = embedding_layer_input(encoder_inputs)\n",
        "encoder = LSTM(latent_dim, return_state=True)\n",
        "encoder_outputs, state_h, state_c = encoder(encoder_embeddings)\n",
        "encoder_states = [state_h, state_c]\n",
        "\n",
        "decoder_inputs = Input(shape=(max_decoder_seq_length,))\n",
        "decoder_embeddings = embedding_layer_target(decoder_inputs)\n",
        "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
        "decoder_outputs, _, _ = decoder_lstm(decoder_embeddings, initial_state=encoder_states)\n",
        "decoder_dense = Dense(num_decoder_tokens, activation='softmax')\n",
        "decoder_outputs = decoder_dense(decoder_outputs)\n",
        "\n",
        "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "\n",
        "optimizer = RMSprop(learning_rate=0.001)\n",
        "model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
        "\n",
        "# Model fitting\n",
        "history = model.fit(\n",
        "    [encoder_input_data_train, decoder_input_data_train], decoder_target_data_train,\n",
        "    batch_size=batch_size,\n",
        "    epochs=epochs,\n",
        "    validation_data=(\n",
        "        [encoder_input_data_val, decoder_input_data_val], decoder_target_data_val\n",
        "    ),\n",
        "    callbacks=[early_stopping]\n",
        ")\n",
        "\n",
        "# Evaluation on test set\n",
        "test_loss, test_accuracy = model.evaluate(\n",
        "    [encoder_input_data_test, decoder_input_data_test], decoder_target_data_test\n",
        ")\n",
        "print(f'Test Loss: {test_loss}')\n",
        "print(f'Test Accuracy: {test_accuracy}')"
      ],
      "metadata": {
        "id": "_nFOFBxBbDPg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7ec1d46b-7db2-4f6f-c5fd-b470e70b776e"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "125/125 [==============================] - 9s 39ms/step - loss: 1.6792 - accuracy: 0.5069 - val_loss: 1.5637 - val_accuracy: 0.7878\n",
            "Epoch 2/100\n",
            "125/125 [==============================] - 3s 23ms/step - loss: 1.4710 - accuracy: 0.6513 - val_loss: 1.4915 - val_accuracy: 0.8000\n",
            "Epoch 3/100\n",
            "125/125 [==============================] - 3s 23ms/step - loss: 1.4150 - accuracy: 0.7855 - val_loss: 1.4536 - val_accuracy: 0.8095\n",
            "Epoch 4/100\n",
            "125/125 [==============================] - 4s 31ms/step - loss: 1.3750 - accuracy: 0.8168 - val_loss: 1.4300 - val_accuracy: 0.8103\n",
            "Epoch 5/100\n",
            "125/125 [==============================] - 3s 24ms/step - loss: 1.3449 - accuracy: 0.8207 - val_loss: 1.4040 - val_accuracy: 0.8172\n",
            "Epoch 6/100\n",
            "125/125 [==============================] - 3s 23ms/step - loss: 1.3222 - accuracy: 0.8233 - val_loss: 1.3790 - val_accuracy: 0.8199\n",
            "Epoch 7/100\n",
            "125/125 [==============================] - 3s 23ms/step - loss: 1.3012 - accuracy: 0.8266 - val_loss: 1.3616 - val_accuracy: 0.8197\n",
            "Epoch 8/100\n",
            "125/125 [==============================] - 4s 30ms/step - loss: 1.2834 - accuracy: 0.8280 - val_loss: 1.3585 - val_accuracy: 0.8221\n",
            "Epoch 9/100\n",
            "125/125 [==============================] - 3s 25ms/step - loss: 1.2687 - accuracy: 0.8299 - val_loss: 1.3318 - val_accuracy: 0.8224\n",
            "Epoch 10/100\n",
            "125/125 [==============================] - 3s 23ms/step - loss: 1.2538 - accuracy: 0.8317 - val_loss: 1.3312 - val_accuracy: 0.8261\n",
            "Epoch 11/100\n",
            "125/125 [==============================] - 3s 22ms/step - loss: 1.2417 - accuracy: 0.8334 - val_loss: 1.3477 - val_accuracy: 0.8256\n",
            "Epoch 12/100\n",
            "125/125 [==============================] - 3s 27ms/step - loss: 1.2315 - accuracy: 0.8345 - val_loss: 1.3154 - val_accuracy: 0.8273\n",
            "Epoch 13/100\n",
            "125/125 [==============================] - 3s 27ms/step - loss: 1.2194 - accuracy: 0.8347 - val_loss: 1.3147 - val_accuracy: 0.8254\n",
            "Epoch 14/100\n",
            "125/125 [==============================] - 3s 24ms/step - loss: 1.2113 - accuracy: 0.8357 - val_loss: 1.3099 - val_accuracy: 0.8274\n",
            "Epoch 15/100\n",
            "125/125 [==============================] - 3s 23ms/step - loss: 1.2018 - accuracy: 0.8370 - val_loss: 1.3000 - val_accuracy: 0.8295\n",
            "Epoch 16/100\n",
            "125/125 [==============================] - 3s 23ms/step - loss: 1.1927 - accuracy: 0.8369 - val_loss: 1.2999 - val_accuracy: 0.8271\n",
            "Epoch 17/100\n",
            "125/125 [==============================] - 4s 32ms/step - loss: 1.1849 - accuracy: 0.8381 - val_loss: 1.2990 - val_accuracy: 0.8269\n",
            "Epoch 18/100\n",
            "125/125 [==============================] - 3s 23ms/step - loss: 1.1776 - accuracy: 0.8316 - val_loss: 1.2787 - val_accuracy: 0.8294\n",
            "Epoch 19/100\n",
            "125/125 [==============================] - 3s 23ms/step - loss: 1.1705 - accuracy: 0.8322 - val_loss: 1.2793 - val_accuracy: 0.8291\n",
            "Epoch 20/100\n",
            "125/125 [==============================] - 3s 23ms/step - loss: 1.1636 - accuracy: 0.8263 - val_loss: 1.2715 - val_accuracy: 0.8292\n",
            "Epoch 21/100\n",
            "125/125 [==============================] - 3s 27ms/step - loss: 1.1572 - accuracy: 0.8164 - val_loss: 1.2721 - val_accuracy: 0.8304\n",
            "Epoch 22/100\n",
            "125/125 [==============================] - 3s 27ms/step - loss: 1.1513 - accuracy: 0.8283 - val_loss: 1.2856 - val_accuracy: 0.8302\n",
            "Epoch 23/100\n",
            "125/125 [==============================] - 3s 23ms/step - loss: 1.1460 - accuracy: 0.8274 - val_loss: 1.2644 - val_accuracy: 0.8236\n",
            "Epoch 24/100\n",
            "125/125 [==============================] - 3s 23ms/step - loss: 1.1391 - accuracy: 0.8354 - val_loss: 1.2571 - val_accuracy: 0.8182\n",
            "Epoch 25/100\n",
            "125/125 [==============================] - 3s 23ms/step - loss: 1.1351 - accuracy: 0.8318 - val_loss: 1.2640 - val_accuracy: 0.8297\n",
            "Epoch 26/100\n",
            "125/125 [==============================] - 4s 30ms/step - loss: 1.1287 - accuracy: 0.8355 - val_loss: 1.2604 - val_accuracy: 0.8310\n",
            "Epoch 27/100\n",
            "125/125 [==============================] - 3s 23ms/step - loss: 1.1247 - accuracy: 0.8180 - val_loss: 1.2587 - val_accuracy: 0.8231\n",
            "32/32 [==============================] - 1s 19ms/step - loss: 1.2469 - accuracy: 0.8215\n",
            "Test Loss: 1.2469173669815063\n",
            "Test Accuracy: 0.8215000033378601\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4oXyrqvHaaDQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}